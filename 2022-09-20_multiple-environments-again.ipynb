{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2024\n",
    "\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "test_epochs = 10\n",
    "use_l1 = False\n",
    "\n",
    "environment_name = 'cifar10'\n",
    "# options:\n",
    "# - cifar10\n",
    "# - mnist\n",
    "# - spotify # doesn't currently work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete,Box\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import clone_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "\n",
    "from environment import MnistEnv,Cifar10Env,SpotifyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.414\n",
    "\n",
    "def indicator(x,y):\n",
    "    if x == y:\n",
    "        return 0.\n",
    "    else:\n",
    "        return 2.\n",
    "\n",
    "def kernel(x,y):\n",
    "    return np.exp((-1.) * np.power(gamma,-2) * indicator(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc952b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_environment(env_name = 'mnist'):\n",
    "    if env_name == 'mnist':\n",
    "        return MnistEnv()\n",
    "    elif env_name == 'cifar10':\n",
    "        return Cifar10Env()\n",
    "    elif env_name == 'spotify':\n",
    "        return SpotifyEnv()\n",
    "    else:\n",
    "        print('Nope! That does not exist!')\n",
    "        print('You will get MNIST!')\n",
    "        return MnistEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e513133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fcn(model,previous_model,states,mask,rewards,loss_type = 'q', use_entropy = False, use_l1 = False, use_mmd = False):\n",
    "    \n",
    "    if loss_type == 'q':\n",
    "    \n",
    "        preds = model(np.array(states))\n",
    "        q_values = tf.reduce_sum(preds * mask, axis=1, keepdims=True)\n",
    "        loss = mean_squared_error(q_values, rewards)\n",
    "        alg_loss = loss.numpy()\n",
    "        \n",
    "        # Calculate entropy\n",
    "        policy = tf.nn.softmax(preds)\n",
    "        entropy = tf.reduce_sum((-1) * tf.math.log(policy + 1e-8) * policy)\n",
    "        if use_entropy:\n",
    "            loss -= 0.01 * entropy\n",
    "        \n",
    "        # Calculate L1 Norm\n",
    "        if use_l1:\n",
    "            loss += 0.001 * tf.norm(model.trainable_weights, ord=1)\n",
    "        \n",
    "        # Calculate MMD\n",
    "        policy = tf.nn.softmax(preds)\n",
    "\n",
    "        prev_preds = previous_model(np.array(states))\n",
    "        prev_policy = tf.nn.softmax(preds)\n",
    "        # Edited\n",
    "        # Forcing uniform distribution\n",
    "        prev_policy = tf.convert_to_tensor((1/env.action_space.n) * np.ones(shape = prev_policy.numpy().shape))\n",
    "\n",
    "        prob_products = np.array([np.asmatrix(prev_policy.numpy()[s]).transpose() * np.asmatrix(policy.numpy()[s]) for s in range(len(states))])\n",
    "        mmd_coefs = np.array([[(kernel_coefs[a] * prob_products[s]).sum() for a in range(10)] for s in range(len(prob_products))])\n",
    "        mmd_loss = tf.reduce_sum(mmd_coefs * policy)\n",
    "        if use_mmd:\n",
    "            loss += 0.01 * mmd_loss\n",
    "            \n",
    "        \n",
    "    elif loss_type == 'po':\n",
    "        \n",
    "        rewards = tf.reduce_sum(rewards,axis = 1,keepdims = True)\n",
    "        preds = model(np.array(states))\n",
    "        policy = tf.nn.softmax(preds)\n",
    "        probs = tf.reduce_sum(policy * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean((-1) * rewards * probs)\n",
    "        alg_loss = loss.numpy()\n",
    "        \n",
    "        # Calculate entropy\n",
    "        entropy = tf.reduce_sum((-1) * tf.math.log(policy + 1e-6) * policy)\n",
    "        if use_entropy:\n",
    "            loss -= 0.01 * entropy\n",
    "        \n",
    "        # Calculate L1 Norm\n",
    "        if use_l1:\n",
    "            loss += 0.001 * tf.norm(model.trainable_weights, ord=1)\n",
    "        \n",
    "        # Calculate MMD\n",
    "        prev_preds = previous_model(np.array(states))\n",
    "        prev_policy = tf.nn.softmax(preds)\n",
    "        # Edited\n",
    "        # Forcing uniform distribution\n",
    "        prev_policy = tf.convert_to_tensor((1/env.action_space.n) * np.ones(shape = prev_policy.numpy().shape))\n",
    "\n",
    "        prob_products = np.array([np.asmatrix(prev_policy.numpy()[s]).transpose() * np.asmatrix(policy.numpy()[s]) for s in range(len(states))])\n",
    "        mmd_coefs = np.array([[(np.multiply(kernel_coefs[a],prob_products[s])).sum() for a in range(10)] for s in range(len(prob_products))])\n",
    "        mmd_loss = tf.reduce_sum(mmd_coefs * policy)\n",
    "        if use_mmd:\n",
    "            loss += 0.01 * mmd_loss\n",
    "\n",
    "    elif loss_type == 'pg':\n",
    "        \n",
    "        rewards = tf.reduce_sum(rewards,axis = 1,keepdims = True)\n",
    "        preds = model(np.array(states))\n",
    "        policy = tf.nn.softmax(preds)\n",
    "        probs = tf.reduce_sum(policy * mask, axis=1, keepdims=True)\n",
    "        log_probs = tf.math.log(probs + 1e-6)\n",
    "        loss = tf.reduce_mean((-1) * rewards * log_probs)\n",
    "        alg_loss = loss.numpy()\n",
    "        \n",
    "        # Calculate entropy\n",
    "        entropy = tf.reduce_sum((-1) * tf.math.log(policy + 1e-6) * policy)\n",
    "        if use_entropy:\n",
    "            loss -= 0.01 * entropy\n",
    "        \n",
    "        # Calculate L1 Norm\n",
    "        if use_l1:\n",
    "            loss += 0.001 * tf.norm(model.trainable_weights, ord=1)\n",
    "        \n",
    "        # Calculate MMD\n",
    "        prev_preds = previous_model(np.array(states))\n",
    "        prev_policy = tf.nn.softmax(preds)\n",
    "        # Edited\n",
    "        # Forcing uniform distribution\n",
    "        prev_policy = tf.convert_to_tensor((1/env.action_space.n) * np.ones(shape = prev_policy.numpy().shape))\n",
    "\n",
    "        prob_products = np.array([np.asmatrix(prev_policy.numpy()[s]).transpose() * np.asmatrix(policy.numpy()[s]) for s in range(len(states))])\n",
    "        mmd_coefs = np.array([[(np.multiply(kernel_coefs[a],prob_products[s])).sum() for a in range(10)] for s in range(len(prob_products))])\n",
    "        mmd_loss = tf.reduce_sum(mmd_coefs * policy)\n",
    "        if use_mmd:\n",
    "            loss += 0.01 * mmd_loss\n",
    "    \n",
    "    elif loss_type == 'po_escort':\n",
    "        \n",
    "        rewards = tf.reduce_sum(rewards,axis = 1,keepdims = True)\n",
    "        preds = model(np.array(states))\n",
    "        numerator = tf.math.pow(tf.math.abs(preds), 2)\n",
    "        denominator = tf.reduce_sum(numerator, axis = 1, keepdims = True)\n",
    "        policy = tf.math.divide(numerator, denominator)\n",
    "        probs = tf.reduce_sum(policy * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean((-1) * rewards * probs)\n",
    "    \n",
    "    return loss,alg_loss,entropy,mmd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape = env.reset().shape)) #(1,) + env.reset().shape))\n",
    "\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "\n",
    "    model.add(Dense(10, activation = 'linear', use_bias = False))\n",
    "\n",
    "    model.compile()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d477717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model_name = 'linear',\n",
    "    num_epochs = 10000,\n",
    "    batch_size = 32,\n",
    "    loss_type = 'po',\n",
    "    use_entropy = False,\n",
    "    use_l1 = False,\n",
    "    use_mmd = False,\n",
    "    test_epochs = 20, # number of times to save test data (after initialization)\n",
    "    temperature = 1.0\n",
    "):\n",
    "    \n",
    "    if use_entropy:\n",
    "        entropy_used = '_entropy'\n",
    "    else:\n",
    "        entropy_used = ''\n",
    "    \n",
    "    if use_l1:\n",
    "        l1_used = '_l1'\n",
    "    else:\n",
    "        l1_used = ''\n",
    "    \n",
    "    if use_mmd:\n",
    "        mmd_used = '_mmd'\n",
    "    else:\n",
    "        mmd_used = ''\n",
    "    \n",
    "    model = build_model()\n",
    "    previous_model = clone_model(model)\n",
    "\n",
    "    if loss_type == 'q':\n",
    "        replay_buffer = deque(maxlen = 10000)\n",
    "        temperature = 10.\n",
    "    else:\n",
    "        replay_buffer = deque(maxlen = batch_size)\n",
    "        \n",
    "    optimizer = Adam(learning_rate = 0.0001)\n",
    "\n",
    "    # index = random.sample(list(range(len(x_test))), 1000)\n",
    "    index = list(range(len(x_test)))\n",
    "    x_test_small = [x_test[i] for i in index]\n",
    "    y_test_small = [y_test[i] for i in index]\n",
    "    train_losses = []\n",
    "    train_alg_losses = []\n",
    "    train_entropy = []\n",
    "    train_mmd = []\n",
    "    test_losses = []\n",
    "    test_actions = []\n",
    "    test_actions_random = []\n",
    "\n",
    "    # Initial predictions\n",
    "    preds = model(np.array(x_test_small))\n",
    "    actions = [np.argmax(p) for p in preds]\n",
    "    test_losses.append(accuracy_score(actions,y_test_small))\n",
    "    test_actions.append(np.histogram(actions, bins = list(range(env.action_space.n + 1)), density = True)[0])\n",
    "    actions = tf.random.categorical(tf.math.log(tf.nn.softmax(temperature * preds)), 1)\n",
    "    test_actions_random.append(np.histogram(actions, bins = list(range(env.action_space.n + 1)), density = True)[0])\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs + batch_size), desc = f'{loss_type}{entropy_used}{l1_used}{mmd_used}', leave = False):\n",
    "\n",
    "        obs = env.reset()\n",
    "        obs = obs.reshape((1,) + obs.shape)\n",
    "\n",
    "        pred = model.predict(obs)\n",
    "        action = tf.random.categorical(tf.math.log(tf.nn.softmax(temperature * pred)), 1)\n",
    "        _,reward,_,_ = env.step(action)\n",
    "        replay_buffer.append((obs,action,reward))\n",
    "\n",
    "        # Compute loss and train if memory buff is big enough\n",
    "        if len(replay_buffer) < batch_size:\n",
    "                pass\n",
    "        else:\n",
    "            sample = random.sample(replay_buffer,batch_size)\n",
    "            states,actions,rewards = list(zip(*sample))\n",
    "            mask = tf.one_hot([a.numpy().item() for a in actions],10)\n",
    "            rewards = np.expand_dims(rewards, axis=1) * mask\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss,alg_loss,entropy,mmd = loss_fcn(model,previous_model,states,mask,rewards,loss_type, use_entropy, use_l1, use_mmd)\n",
    "            train_losses.append(loss.numpy())\n",
    "            train_alg_losses.append(alg_loss)\n",
    "            train_entropy.append(entropy.numpy())\n",
    "            train_mmd.append(mmd.numpy())\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            previous_model = clone_model(model)\n",
    "\n",
    "            if ((epoch - (batch_size - 1)) % (num_epochs // test_epochs) == 0) and (epoch > batch_size - 1):\n",
    "                \n",
    "                preds = model(np.array(x_test_small))\n",
    "                \n",
    "                # Greedy action selection\n",
    "                actions = [np.argmax(p) for p in preds]\n",
    "                test_losses.append(accuracy_score(actions,y_test_small))\n",
    "                test_actions.append(np.histogram(actions, bins = list(range(env.action_space.n + 1)), density = True)[0])\n",
    "                \n",
    "                # Stochastic action selection\n",
    "                actions = tf.random.categorical(tf.math.log(tf.nn.softmax(temperature * preds)), 1)\n",
    "                test_actions_random.append(np.histogram(actions, bins = list(range(env.action_space.n + 1)), density = True)[0])\n",
    "\n",
    "    model.save(f'models/{environment_name}_{model_name}_{loss_type}{entropy_used}{l1_used}{mmd_used}_{seed}.h5')\n",
    "    \n",
    "    temp = pd.DataFrame(\n",
    "        data = test_losses,\n",
    "        index = [(num_epochs // test_epochs) * i for i in range(1 + test_epochs)]\n",
    "    ).to_csv(f'data/{environment_name}_{model_name}_testlosses_{loss_type}{entropy_used}{l1_used}{mmd_used}_{num_epochs}_{seed}.csv')\n",
    "    temp = pd.DataFrame(\n",
    "        data = train_losses\n",
    "    ).to_csv(f'data/{environment_name}_{model_name}_trainlosses_{loss_type}{entropy_used}{l1_used}{mmd_used}_{num_epochs}_{seed}.csv')\n",
    "    temp = pd.DataFrame(\n",
    "        data = train_alg_losses\n",
    "    ).to_csv(f'data/{environment_name}_{model_name}_trainalglosses_{loss_type}{entropy_used}{l1_used}{mmd_used}_{num_epochs}_{seed}.csv')\n",
    "    temp = pd.DataFrame(\n",
    "        data = train_entropy\n",
    "    ).to_csv(f'data/{environment_name}_{model_name}_trainentropy_{loss_type}{entropy_used}{l1_used}{mmd_used}_{num_epochs}_{seed}.csv')\n",
    "    temp = pd.DataFrame(\n",
    "        data = train_mmd\n",
    "    ).to_csv(f'data/{environment_name}_{model_name}_trainmmd_{loss_type}{entropy_used}{l1_used}{mmd_used}_{num_epochs}_{seed}.csv')\n",
    "    \n",
    "    return train_losses,test_losses,test_actions,test_actions_random\n",
    "\n",
    "def create_plots(model_name,test_actions,test_actions_random,num_epochs,test_epochs,loss_type,use_entropy = False,use_l1 = False,use_mmd = False):\n",
    "    \n",
    "    title = f'Action Selections for {environment_name} Test Set\\nLoss: {loss_type.upper()}'\n",
    "    \n",
    "    if use_entropy:\n",
    "        entropy_used = '_entropy'\n",
    "        if use_l1:\n",
    "            title += ' with entropy and l1 regularization'\n",
    "        else:\n",
    "            title += ' with entropy regularization'\n",
    "    else:\n",
    "        entropy_used = ''\n",
    "        if use_l1:\n",
    "            title += ' with l1 regularization'\n",
    "    \n",
    "    if use_l1:\n",
    "        l1_used = '_l1'\n",
    "    else:\n",
    "        l1_used = ''\n",
    "        \n",
    "    if use_mmd:\n",
    "        mmd_used = '_mmd'\n",
    "        title += ' with MMD regularization'\n",
    "    else:\n",
    "        mmd_used = ''\n",
    "\n",
    "    # Greedy\n",
    "    df = pd.DataFrame(\n",
    "        data = test_actions,\n",
    "        index = [(num_epochs // test_epochs) * i for i in range(1 + test_epochs)]\n",
    "    )\n",
    "\n",
    "    df.to_csv(f'data/{environment_name}_{model_name}_{loss_type}{entropy_used}{l1_used}{mmd_used}_{num_epochs}_{seed}.csv')\n",
    "\n",
    "    fig = plt.figure(figsize = (10,5))\n",
    "    ax = plt.subplot(111)\n",
    "    df.plot.bar(stacked = True, ax = ax, width = 1)#, width = 0.9)\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5), loc = 'center left', title = 'Action')\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    ax.set_xlabel('Training Iteration')\n",
    "    ax.set_ylabel('Selection Percentages')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.savefig(f'images/{environment_name}_{model_name}_{loss_type}{entropy_used}{l1_used}{mmd_used}_{num_epochs}_{seed}.png', dpi = 250)\n",
    "    \n",
    "    # Stochastic\n",
    "    df = pd.DataFrame(\n",
    "        data = test_actions_random,\n",
    "        index = [(num_epochs // test_epochs) * i for i in range(1 + test_epochs)]\n",
    "    )\n",
    "\n",
    "    df.to_csv(f'data/{environment_name}_{model_name}_random_{loss_type}{entropy_used}{l1_used}{mmd_used}_{num_epochs}_{seed}.csv')\n",
    "\n",
    "    fig = plt.figure(figsize = (10,5))\n",
    "    ax = plt.subplot(111)\n",
    "    df.plot.bar(stacked = True, ax = ax, width = 1)#, width = 0.9)\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5), loc = 'center left', title = 'Action')\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    ax.set_xlabel('Training Iteration')\n",
    "    ax.set_ylabel('Stochastic Selection Percentages')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.savefig(f'images/{environment_name}_{model_name}_random_{loss_type}{entropy_used}{l1_used}{mmd_used}_{num_epochs}_{seed}.png', dpi = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_type in tqdm(['po','pg','q'], desc = 'Losses'):\n",
    "\n",
    "    for (use_entropy,use_mmd) in tqdm(\n",
    "        [(False,False),(True,False),(False,True)],\n",
    "        desc = 'Regularizers'\n",
    "    ):\n",
    "\n",
    "        env = set_environment(environment_name)\n",
    "\n",
    "        # Set seed\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.seed(seed)\n",
    "\n",
    "        kernel_coefs = np.array([\n",
    "            [\n",
    "                [\n",
    "                    kernel(a,a_prime) - kernel(a,a_star) for a_prime in range(10)\n",
    "                ] for a_star in range(10)\n",
    "            ] for a in range(10)\n",
    "        ])\n",
    "\n",
    "        x_test,y_test = env.test_set()\n",
    "\n",
    "        train_losses,test_losses,test_actions,test_actions_random = train_model(\n",
    "            '2layers',\n",
    "            num_epochs,\n",
    "            batch_size,\n",
    "            loss_type,\n",
    "            use_entropy,\n",
    "            use_l1,\n",
    "            use_mmd,\n",
    "            test_epochs\n",
    "        )\n",
    "\n",
    "        create_plots(\n",
    "            '2layers',\n",
    "            test_actions,\n",
    "            test_actions_random,\n",
    "            num_epochs,\n",
    "            test_epochs,\n",
    "            loss_type,\n",
    "            use_entropy,\n",
    "            use_l1,\n",
    "            use_mmd\n",
    "        )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
